{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This file illustrates how you might experiment with the HMM interface.\n", "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n", "A notebook interface is nicer than the plain Python prompt, so we provide\n", "a notebook version of this file as `test_en.ipynb`, which you can open with\n", "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging\n", "import math\n", "import os\n", "from pathlib import Path\n", "from typing import Callable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from corpus import TaggedCorpus\n", "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n", "from hmm import HiddenMarkovModel\n", "from lexicon import build_lexicon\n", "import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up logging."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n", "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG\n", "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Switch working directory to the directory where the data live.  You may need to edit this line."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.chdir(\"../data\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n", "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n", "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n", "log.info(f\"Tagset: f{list(entrain.tagset)}\")\n", "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make an HMM.  Let's do supervised pre-training to approximately\n", "maximize the regularized log-likelihood.  If you want to speed this\n", "up, you can increase the tolerance of training (using the\n", "`tolerance` argument), since we don't really have to train to\n", "convergence.\n", "\n", "We arbitrarily choose `reg=1`, but it would be better to search\n", "for the best regularization strength."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lexicon = build_lexicon(entrain, embeddings_file=Path('words-50.txt'))  # works better with more dims!\n", "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab, lexicon)  # randomly initialized parameters\n", "loss_sup = lambda model: model_cross_entropy(model, eval_corpus=ensup)\n", "hmm.train(corpus=ensup, loss=loss_sup, \n", "          minibatch_size=30, evalbatch_size=10000, \n", "          reg=1, lr=0.0001, save_path=\"ensup_hmm.pkl\") "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's throw in the unsupervised training data as well, and continue\n", "training to try to improve accuracy on held-out development data.\n", "We'll stop when this accuracy stops getting better.\n", "\n", "This step is delicate, so we'll use a much smaller learning rate and\n", "pause to evaluate more often, in hopes that tagging accuracy will go\n", "up for a little bit before it goes down again (see Merialdo 1994).\n", "(Log-likelihood will continue to improve, just not accuracy.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hmm = HiddenMarkovModel.load(\"ensup_hmm.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n", "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n", "                                            known_vocab=known_vocab)\n", "hmm.train(corpus=entrain, loss=loss_dev,\n", "          minibatch_size=30, evalbatch_size=len(entrain)//4, # evaluate 4 times per epoch\n", "          reg=1, lr=0.000001, save_path=\"entrain_hmm.pkl\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can also retry the above workflow where you start with a worse\n", "supervised model (like Merialdo).  Replace `ensup` throughout the\n", "corpus setup with `ensup-tiny`, which is only 25 sentences (that\n", "cover all tags in `endev`).  And change the names of your saved\n", "models."]}, {"cell_type": "markdown", "metadata": {}, "source": ["More detailed look at the first 10 sentences in the held-out corpus,\n", "including Viterbi tagging."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for m, sentence in enumerate(endev):\n", "    if m >= 10: break\n", "    viterbi = hmm.viterbi_tagging(sentence.desupervise(), endev)\n", "    counts = eval_tagging(predicted=viterbi, gold=sentence, \n", "                          known_vocab=known_vocab)\n", "    num = counts['NUM', 'ALL']\n", "    denom = counts['DENOM', 'ALL']\n", "    \n", "    log.info(f\"Gold:    {sentence}\")\n", "    log.info(f\"Viterbi: {viterbi}\")\n", "    log.info(f\"Loss:    {denom - num}/{denom}\")\n", "    log.info(f\"Prob:    {math.exp(hmm.log_prob(sentence, endev))}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}
