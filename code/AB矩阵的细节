hmm.py中AB矩阵的一些解释

1.AB矩阵的形状

super().__init__() # type: ignore # pytorch nn.Module does not have type annotations

# We'll use the variable names that we used in the reading handout, for
# easy reference.  (It's typically good practice to use more descriptive names.)
# As usual in Python, attributes starting with _ are intended as private;
# in this case, they might go away if you changed the parametrization of the model.

# We omit EOS_WORD and BOS_WORD from the vocabulary, as they can never be emitted.
# See the reading handout section "Don't guess when you know."

assert vocab[-2:] == [EOS_WORD, BOS_WORD]  # make sure these are the last two

self.k = len(tagset)       # number of tag types
self.V = len(vocab) - 2    # number of word types (not counting EOS_WORD and BOS_WORD)
self.d = lexicon.size(1)   # dimensionality of a word's embedding in attribute space
self.unigram = unigram     # do we fall back to a unigram model?

self.tagset = tagset
self.vocab = vocab
self._E = lexicon[:-2]  # embedding matrix; omits rows for EOS_WORD and BOS_WORD


def init_params(self) -> None:
    """Initialize params to small random values (which breaks ties in the fully unsupervised case).
    However, we initialize the BOS_TAG column of _WA to -inf, to ensure that
    we have 0 probability of transitioning to BOS_TAG (see "Don't guess when you know").
    See the "Parametrization" section of the reading handout."""

    # See the reading handout section "Parametrization."
    #

    # As in HW3's probs.py, our model instance's parameters are tensors
    # that have been wrapped in nn.Parameter.  That wrapper produces a
    # new view of each tensor with requires_grad=True.  It also ensures
    # that they are included in self.parameters(), which this class
    # inherits from nn.Module and which is used below for
    # regularization and training.

    ThetaB = 0.01*torch.rand(self.k, self.d)
    self._ThetaB = nn.Parameter(ThetaB)    # params used to construct emission matrix

    WA = 0.01*torch.rand(1 if self.unigram # just one row if unigram model
                         else self.k,      # but one row per tag s if bigram model
                         self.k)           # one column per tag t
    WA[:, self.bos_t] = -inf               # correct the BOS_TAG column
    self._WA = nn.Parameter(WA)            # params used to construct transition matrix


  A是k*k；B是k*d
  k是标签个数 包括bos和eos
  d是embedding维数

  1.AB矩阵的特殊处理
  def init_params(self) -> None:
    """Initialize params to small random values (which breaks ties in the fully unsupervised case).
    However, we initialize the BOS_TAG column of _WA to -inf, to ensure that
    we have 0 probability of transitioning to BOS_TAG (see "Don't guess when you know").
    See the "Parametrization" section of the reading handout."""

    # See the reading handout section "Parametrization."
    #

    # As in HW3's probs.py, our model instance's parameters are tensors
    # that have been wrapped in nn.Parameter.  That wrapper produces a
    # new view of each tensor with requires_grad=True.  It also ensures
    # that they are included in self.parameters(), which this class
    # inherits from nn.Module and which is used below for
    # regularization and training.

    ThetaB = 0.01*torch.rand(self.k, self.d)
    self._ThetaB = nn.Parameter(ThetaB)    # params used to construct emission matrix

    WA = 0.01*torch.rand(1 if self.unigram # just one row if unigram model
                         else self.k,      # but one row per tag s if bigram model
                         self.k)           # one column per tag t
    WA[:, self.bos_t] = -inf               # correct the BOS_TAG column
    self._WA = nn.Parameter(WA)            # params used to construct transition matrix


def updateAB(self) -> None:
"""Set the transition and emission matrices A and B, based on the current parameters.
See the "Parametrization" section of the reading handout."""

A = F.softmax(self._WA, dim=1)       # run softmax on params to get transition distributions
                                     # note that the BOS_TAG column will be 0, but each row will sum to 1
if self.unigram:
    # A is a row vector giving unigram probabilities p(t).
    # We'll just set the bigram matrix to use these as p(t | s)
    # for every row s.  This lets us simply use the bigram
    # code for unigram experiments, although unfortunately that
    # preserves the O(nk^2) runtime instead of letting us speed
    # up to O(nk) in the unigram case.
    self.A = A.repeat(self.k, 1)
else:
    # A is already a full matrix giving p(t | s).
    self.A = A

WB = self._ThetaB @ self._E.t()  # inner products of tag weights and word embeddings
B = F.softmax(WB, dim=1)         # run softmax on those inner products to get emission distributions
self.B = B.clone()
self.B[self.eos_t, :] = 0        # but don't guess: EOS_TAG can't emit any column's word (only EOS_WORD)
self.B[self.bos_t, :] = 0        # same for BOS_TAG (although BOS_TAG will already be ruled out by other factors)

A矩阵的bos-t列被特殊处理 没有标签能转移到bos-t
B矩阵的eos-t和bos-t行被处理 因为这两个标签有固定的发射
