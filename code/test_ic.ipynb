{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This file illustrates how you might experiment with the HMM interface.\n", "You can paste these commands in at the Python prompt, or execute `test_ic.py` directly.\n", "A notebook interface is nicer than the plain Python prompt, so we provide\n", "a notebook version of this file as `test_ic.ipynb`, which you can open with\n", "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging, math, os\n", "from pathlib import Path\n", "from typing import Callable"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from corpus import TaggedCorpus\n", "from eval import model_cross_entropy, write_tagging\n", "from hmm import HiddenMarkovModel\n", "from lexicon import build_lexicon\n", "import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up logging."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log = logging.getLogger(\"test_ic\")       # For usage, see findsim.py in earlier assignment.\n", "logging.basicConfig(level=logging.INFO)  # could change INFO to DEBUG\n", "# torch.autograd.set_detect_anomaly(True)    # uncomment to improve error messages from .backward(), but slows down"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Switch working directory to the directory where the data live.  You may want to edit this line."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.chdir(\"../data\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make an HMM with randomly initialized parameters."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["icsup = TaggedCorpus(Path(\"icsup\"), add_oov=False)\n", "log.info(f\"Ice cream vocabulary: {list(icsup.vocab)}\")\n", "log.info(f\"Ice cream tagset: {list(icsup.tagset)}\")\n", "lexicon = build_lexicon(icsup, one_hot=True)   # one-hot lexicon: separate parameters for each word\n", "hmm = HiddenMarkovModel(icsup.tagset, icsup.vocab, lexicon)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** Current A, B matrices (computed by softmax from small random parameters)\")\n", "hmm.updateAB()   # compute the matrices from the initial parameters (this would normally happen during training).\n", "                 # An alternative is to set them directly to some spreadsheet values you'd like to try.\n", "hmm.printAB()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["While training on ice cream, we will just evaluate the cross-entropy\n", "on the training data itself (icsup), since we are interested in watching it improve."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** Supervised training on icsup\")\n", "cross_entropy_loss = lambda model: model_cross_entropy(model, icsup)\n", "hmm.train(corpus=icsup, loss=cross_entropy_loss, \n", "          minibatch_size=10, evalbatch_size=500, lr=0.01, tolerance=0.0001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** A, B matrices after training on icsup (should approximately \"\n", "         \"match initial params on spreadsheet [transposed])\")\n", "hmm.printAB()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Since we used a low tolerance, that should have gotten us about up to the\n", "initial parameters on the spreadsheet.  Let's tag the spreadsheet \"sentence\"\n", "(that is, the sequence of ice creams) using the Viterbi algorithm."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** Viterbi results on icraw\")\n", "icraw = TaggedCorpus(Path(\"icraw\"), tagset=icsup.tagset, vocab=icsup.vocab)\n", "write_tagging(hmm, icraw, Path(\"icraw.output\"))  # calls hmm.viterbi_tagging on each sentence\n", "os.system(\"cat icraw.output\")   # print the file we just created, and remove it"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's use the forward algorithm to see what the model thinks about \n", "the probability of the spreadsheet \"sentence.\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** Forward algorithm on icraw (should approximately match iteration 0 \"\n", "             \"on spreadsheet)\")\n", "for sentence in icraw:\n", "    prob = math.exp(hmm.log_prob(sentence, icraw))\n", "    log.info(f\"{prob} = p({sentence})\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, let's reestimate on the icraw data, as the spreadsheet does."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** Reestimating on icraw (perplexity should improve on every iteration)\")\n", "negative_log_likelihood = lambda model: model_cross_entropy(model, icraw)  # evaluate on icraw itself\n", "hmm.train(corpus=icraw, loss=negative_log_likelihood,\n", "          minibatch_size=10, evalbatch_size=500, lr=0.001, tolerance=0.0001)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["log.info(\"*** A, B matrices after reestimation on icraw (SGD, not EM, but still \"\n", "         \"should approximately match final params on spreadsheet [transposed])\")\n", "hmm.printAB()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}
