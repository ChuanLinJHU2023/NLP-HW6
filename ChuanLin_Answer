c = TaggedCorpus(Path("../data/icsup"))c.tagset

1ai
The new prob that day 1 is hot is 0.491
It is roughly 0.5 because the prob of the path starting with HHH is roughly the same as the prob of the path starting with CHH.
Let's call the prob of the first path P_HHH and that of the second P_CHH. The emission prob e and the transition prob q

P_HHH
= q(H|BOS)*e(1|H)*q(H|H)*e(3|H)*q(H|H)*e(3|H)
= 0.5*0.1*0.8*e(3|H)*q(H|H)*e(3|H)

P_HHH
= q(C|BOS)*e(1|C)*q(H|C)*e(3|H)*q(H|H)*e(3|H)
= 0.5*0.7*0.1*e(3|H)*q(H|H)*e(3|H)

They are roughly the same

1aii
Before change, the prob of day2 being hot is 0.977
After change, the prob of day2 being hot is 0.918
The prob of day2 being hot decreases a little as the number of ice cream of day1 changes from 2 to 1
The cell is K28

1aiii
The most obvious change is that P(H) remains high on the first 7 days before change while P(H) increase from 0 to almost 1 on the first 7 days after change.

1bi
The most obvious change is that P(H) of day11 severly drops from 0.752 to 0. Besides, P(H) of days around day11 also drop.

1bii
The final graph doesn't change much.

1biii
P(1|H) is 1.6E-04 on 10th iteration before change.
P(1|H) is 0 on 10th iteration after change.

P(1|H) remains 0 in each iteration.
We can use counter-proof to easily show this point.
Assume that at a certain Expectation Step, our P(1|H) becomes positive for the first time. We call this Exp Step En
Then we can infer that at the Max Step before this Exp Step, count(h->1) is positive. We call this Max Step M(n-1)
Then we can infer that at the Exp Step before this Max Step, P(1|H) is positive (Otherwise, P(1|H) is 0 and count(H->1) is also 0). We call this Exp Step E(n-1)
However, it is mentioned that at En, P(1|H) becomes positive for the first time, which means that P(1|H) can't be positive at E(n-1)
Thus we have a contradictory

1ci
Note that our subscribe ranges from 0 to n+1
Beta(0, BOS) is the prob of the whole sentence

1cii
For the left tree,
the meaning of a inner node H is that we assign the brother node of this node H, which is a number (1,2,or 3), a tag H.
the meaning of a H constituent is that we assign tags from the position of this H according to the descent nodes of this H node.
The prob of rule H->1 C is the product of the transmission prob and emission prob, namely q(C|H)*e(1|C)
The prob of rule H->e is the transmission prob q(EOS|H)
We prefer the tree on the right because it conforms to CNF.


2a
Note that n is the length of the sentence without counting the BOS and EOS.
BOS is at position 0 while EOS is at position n+1
alpha_BOS(0) = 1 is because that the tag at position 0 must be BOS
bera_EOS(b) = 1 is because that the tag at position n+1 must be EOS

2b
This is because the ground-truth tagging is not always the tagging with lowest perplexity.
On raw file, our HMM always returns the perplexity of the tagging with lowest perplexity
One dev file, our HMM always returns the perplexity of the correct tagging

2c
The purpose of dev is testing, which means that dev can't be seen during training.
The purpose of sup and raw is training (supervised learning and unsupervised learning), which means that they should be used during training.
If we use dev for training, we will overestimate the performance of our HMM during testing.

2d
The backoff of the emission probability limits the performance on known words.
As we know, the emission prob is P(word_i, tag_i).
However, the emission of a word is not only determined by the corresponding tag.
It is also determined by the context words.
Assume that I say "I eat <Noun>". The emission prob of <Noun> is affected by "I eat".
As a result, it will probably emit "fish" or "food", and so on.
We still prefer HMM because it is simple and we can get a parameter estimation with less variance.

2e
The iterations of semi-supervised training does help overall tagging accuracy because the information contained in the partially tagged sentences is still useful.
For example, we can improve our emission prob with semi-supervised learning because the partially-tagged sentences give us a lot of <tag, word> pairs, which improves our frequency estimation of emission.
The tagging accuracy on known words is better than that of seen words, which is better than that of novel words.
This is because there is a lot of information contained in our word embeddings and training dataset, which improves the tagging accuracy.
For novel words, we don't have such a reasonable word embedding in lexicon and statistical information in the training dataset, which decreases our tagging accuracy.

2f
The iterations of semi-supervised training does help overall tagging accuracy because the information contained in the partially tagged sentences is still useful.
For example, we can improve our emission prob with semi-supervised learning because the partially-tagged sentences give us a lot of <tag, word> pairs, which improves our frequency estimation of emission.
Besides, given the tagging of a word, our model can better predict the tagging of the context words, which improves the learning of transition prob and emission prob.
To get the additional value from enraw, we should use it to do unsupervised learning. Besides, we should also partially tag the sentences within it and do semi-supervised learning.
The tagging can be done by human or our current HMM if HMM is pretty sure about the tagging of certain words.

2g
Here are two reasons for why semi-supervised learning may not help.
Firstly, the information of partially tagged sentences overlaps with the fully tagged sentences, which means that our model can't learn something new from these partially tagged sentences.
For example, assume that we train our HMM on fully tagged sentences and then delete some tags of these sentences to get partially tagged sentences and do semi-supervised learning. Then it will probably not help.
Second, the tags in the partially tagged sentences are with a certain bias.
Usually, when a sentence is partially tagged, there is some reason that why some words are tagged while the others are not.
The reason here may contain some bias, which means that the statistical information in partially-tagged sentences may not be consistent with statistical information in real word.
We can only be sure that there is no such bias if partially tagged sentences are obtained by randomly deleting some tags in fully tagged sentences

2h
"enraw + ensup" is a good training strategy that works because it can make our model to benefit from the supervised learning and semi-supervised learning simultaneously.
However, there should be a hyper parameter K to control how much gradient do we receive from ensup compared with from enraw.
The training strategt "enraw + ensup + ensup + ensup" is just the case where K=3.
Besides, the staged training also works.








